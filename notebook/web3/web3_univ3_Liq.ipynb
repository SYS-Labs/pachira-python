{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76e9a8b-11f8-4ece-9d78-35c806004bae",
   "metadata": {},
   "source": [
    "* https://web3-ethereum-defi.readthedocs.io/tutorials/uniswap-v3-liquidity-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2f62315-48f5-49ae-a273-94cb28f94c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from web3 import Web3, HTTPProvider\n",
    "from eth_defi.event_reader.filter import Filter\n",
    "from eth_defi.provider.multi_provider import create_multi_provider_web3\n",
    "\n",
    "# Get your node JSON-RPC URL\n",
    "json_rpc_url='https://mainnet.infura.io/v3/29547b52f77647b68da777e3ecc06811'\n",
    "web3 = create_multi_provider_web3(json_rpc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1015cb44-7fdb-4560-8bd2-540929e94d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eth_defi.uniswap_v3.constants import UNISWAP_V3_FACTORY_CREATED_AT_BLOCK\n",
    "from eth_defi.uniswap_v3.events import fetch_events_to_csv\n",
    "from eth_defi.uniswap_v3.events import get_event_mapping\n",
    "from eth_defi.event_reader.json_state import JSONFileScanState\n",
    "from eth_defi.event_reader.lazy_timestamp_reader import TrackedLazyTimestampReader\n",
    "from eth_defi.event_reader.multithread import MultithreadEventReader\n",
    "from eth_defi.event_reader.logresult import LogContext\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "start_block = UNISWAP_V3_FACTORY_CREATED_AT_BLOCK\n",
    "end_block = UNISWAP_V3_FACTORY_CREATED_AT_BLOCK + 5_000\n",
    "\n",
    "# Stores the last block number of event data we store\n",
    "state = JSONFileScanState(\"/tmp/uniswap-v3-liquidity-scan.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded7fcae-98f6-4aa3-88ad-bd63da980736",
   "metadata": {},
   "outputs": [],
   "source": [
    "web3 = create_multi_provider_web3(json_rpc_url)\n",
    "\n",
    "event_mapping = get_event_mapping(web3)\n",
    "contract_events = [event_data[\"contract_event\"] for event_data in event_mapping.values()]\n",
    "\n",
    "# Create a filter for any Uniswap v3 pool contract, all our events we are interested in\n",
    "filter = Filter.create_filter(address=None, event_types=contract_events)\n",
    "\n",
    "# Start scanning\n",
    "restored, restored_start_block = state.restore_state(start_block)\n",
    "original_block_range = end_block - start_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aee215d-e8b2-4d3a-a419-8897b781197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Uniswap v3 data for block range 12,375,494 - 12,374,621\n"
     ]
    }
   ],
   "source": [
    "buffers = {}\n",
    "output_folder = \"/tmp\"\n",
    "log_info=print\n",
    "max_blocks_once=2000\n",
    "max_threads=10\n",
    "\n",
    "for event_name, mapping in event_mapping.items():\n",
    "    # Each event type gets its own CSV\n",
    "    file_path = f\"{output_folder}/uniswap-v3-{event_name.lower()}.csv\"\n",
    "\n",
    "    exists_already = Path(file_path).exists()\n",
    "    file_handler = open(file_path, \"a\", encoding=\"utf-8\")\n",
    "    csv_writer = csv.DictWriter(file_handler, fieldnames=mapping[\"field_names\"])\n",
    "    if not restored:\n",
    "        headers = \", \".join(mapping[\"field_names\"])\n",
    "        log_info(f\"Creating a new CSV file: {file_path}, with headers: {headers}\")\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "    # For each event, we have its own\n",
    "    # counters and handlers in the context dictionary\n",
    "    buffers[event_name] = {\n",
    "            \"buffer\": [],\n",
    "            \"total\": 0,\n",
    "            \"file_handler\": file_handler,\n",
    "            \"csv_writer\": csv_writer,\n",
    "            \"file_path\": file_path,\n",
    "        }\n",
    "\n",
    "log_info(f\"Saving Uniswap v3 data for block range {restored_start_block:,} - {end_block:,}\")\n",
    "\n",
    "timestamp_reader = TrackedLazyTimestampReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b20a89-efab-43c1-b713-4a41952a2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_mapping = get_event_mapping(web3)\n",
    "contract_events = [event_data[\"contract_event\"] for event_data in event_mapping.values()]\n",
    "\n",
    "# Create a filter for any Uniswap v3 pool contract, all our events we are interested in\n",
    "filter = Filter.create_filter(address=None, event_types=contract_events)\n",
    "\n",
    "# Start scanning\n",
    "restored, restored_start_block = state.restore_state(start_block)\n",
    "original_block_range = end_block - start_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397781d-4bee-4b47-8547-21372f3bddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for event_name, mapping in event_mapping.items():\n",
    "    # Each event type gets its own CSV\n",
    "    file_path = f\"{output_folder}/uniswap-v3-{event_name.lower()}.csv\"\n",
    "\n",
    "    exists_already = Path(file_path).exists()\n",
    "    file_handler = open(file_path, \"a\", encoding=\"utf-8\")\n",
    "    csv_writer = csv.DictWriter(file_handler, fieldnames=mapping[\"field_names\"])\n",
    "    if not restored:\n",
    "        headers = \", \".join(mapping[\"field_names\"])\n",
    "        log_info(f\"Creating a new CSV file: {file_path}, with headers: {headers}\")\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "    # For each event, we have its own\n",
    "    # counters and handlers in the context dictionary\n",
    "    buffers[event_name] = {\n",
    "            \"buffer\": [],\n",
    "            \"total\": 0,\n",
    "            \"file_handler\": file_handler,\n",
    "            \"csv_writer\": csv_writer,\n",
    "            \"file_path\": file_path,\n",
    "        }\n",
    "\n",
    "log_info(f\"Saving Uniswap v3 data for block range {restored_start_block:,} - {end_block:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4f6d5-d923-4efd-9eb5-aa0a757f5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_reader = TrackedLazyTimestampReader()\n",
    "\n",
    "# Wrap everything in a TQDM progress bar, notebook friendly version\n",
    "with tqdm(total=end_block - start_block) as progress_bar:\n",
    "\n",
    "    def update_progress(\n",
    "            current_block,\n",
    "            start_block,\n",
    "            end_block,\n",
    "            chunk_size: int,\n",
    "            total_events: int,\n",
    "            last_timestamp: int,\n",
    "            context: LogContext,\n",
    "    ):\n",
    "        # Update progress bar\n",
    "        #nonlocal buffers\n",
    "    \n",
    "        header_count = timestamp_reader.get_count()\n",
    "    \n",
    "        if last_timestamp:\n",
    "            # Display progress with the date information\n",
    "            d = datetime.datetime.utcfromtimestamp(last_timestamp)\n",
    "            formatted_time = d.strftime(\"%Y-%m-%d\")\n",
    "            progress_bar.set_description(f\"Block: {current_block:,}, events: {total_events:,}, time:{formatted_time}, block headers: {header_count:,}\")\n",
    "        else:\n",
    "            progress_bar.set_description(f\"Block: {current_block:,}, events: {total_events:,}, block headers: {header_count:,}\")\n",
    "    \n",
    "        progress_bar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468cc125-ac67-4641-a03d-33f93c955549",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_reader = TrackedLazyTimestampReader()\n",
    "\n",
    "# Wrap everything in a TQDM progress bar, notebook friendly version\n",
    "with tqdm(total=end_block - start_block) as progress_bar:\n",
    "\n",
    "    def update_progress(\n",
    "            current_block,\n",
    "            start_block,\n",
    "            end_block,\n",
    "            chunk_size: int,\n",
    "            total_events: int,\n",
    "            last_timestamp: int,\n",
    "            context: LogContext,\n",
    "    ):\n",
    "        # Update progress bar\n",
    "        #nonlocal buffers\n",
    "    \n",
    "        header_count = timestamp_reader.get_count()\n",
    "    \n",
    "        if last_timestamp:\n",
    "            # Display progress with the date information\n",
    "            d = datetime.datetime.utcfromtimestamp(last_timestamp)\n",
    "            formatted_time = d.strftime(\"%Y-%m-%d\")\n",
    "            progress_bar.set_description(f\"Block: {current_block:,}, events: {total_events:,}, time:{formatted_time}, block headers: {header_count:,}\")\n",
    "        else:\n",
    "            progress_bar.set_description(f\"Block: {current_block:,}, events: {total_events:,}, block headers: {header_count:,}\")\n",
    "    \n",
    "        progress_bar.update(chunk_size)\n",
    "    \n",
    "        # Update event specific contexes\n",
    "        for buffer_data in buffers.values():\n",
    "            buffer = buffer_data[\"buffer\"]\n",
    "    \n",
    "            # write events to csv\n",
    "            for entry in buffer:\n",
    "                buffer_data[\"csv_writer\"].writerow(entry)\n",
    "                buffer_data[\"total\"] += 1\n",
    "    \n",
    "                # then reset buffer\n",
    "                buffer_data[\"buffer\"] = []\n",
    "    \n",
    "                # Sync the state of updated events\n",
    "        state.save_state(current_block)\n",
    "\n",
    "    # Create a multi-threaded Solidity event reader\n",
    "    # that has a callback to our progress bar notifier\n",
    "    reader = MultithreadEventReader(\n",
    "            json_rpc_url,\n",
    "            notify=update_progress,\n",
    "            max_blocks_once=max_blocks_once,\n",
    "            max_threads=max_threads,\n",
    "        )\n",
    "\n",
    "    for log_result in reader(\n",
    "                    web3,\n",
    "                    restored_start_block,\n",
    "                    end_block,\n",
    "                    filter=filter,\n",
    "                    extract_timestamps=timestamp_reader.extract_timestamps_json_rpc_lazy,\n",
    "                ):\n",
    "                    print('test')\n",
    "                    try:\n",
    "                        event_name = log_result[\"event\"].event_name  # Which event this is: Swap, Burn,...\n",
    "                        buffer = buffers[event_name][\"buffer\"]  # Choose CSV buffer for this event\n",
    "                        decode_function = event_mapping[event_name][\"decode_function\"]  # Each event needs its own decoder\n",
    "                        buffer.append(decode_function(web3, log_result))\n",
    "                    except Exception as e:\n",
    "                        raise RuntimeError(f\"Could not decode {log_result}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daeabab-4689-4e6d-867f-5e64ec25e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close files and print stats\n",
    "for event_name, buffer in buffers.items():\n",
    "    buffer[\"file_handler\"].close()\n",
    "    log_info(f\"Wrote {buffer['total']} {event_name} events to {buffer['file_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760bef7-1d7c-4581-a564-18108f4a2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83191dc-3150-444d-a915-afc3b87f641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/tmp/uniswap-v3-swap.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8169a-e03b-4cab-9ecc-9f5ebae56433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59648661-4768-49cd-9176-e24264c597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eth_defi.uniswap_v3.liquidity import create_tick_delta_csv\n",
    "\n",
    "tick_delta_csv = create_tick_delta_csv(\"/tmp/uniswap-v3-mint.csv\", \"/tmp/uniswap-v3-burn.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
